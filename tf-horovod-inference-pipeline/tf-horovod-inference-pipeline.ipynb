{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TensorFlow Distributed Training & Distributed Inference \n",
    "\n",
    "For use cases involving large datasets, particularly those where the data is images, it often is necessary to perform distributed training on a cluster of multiple machines. Similarly, when it is time to set up an inference workflow, it also may be necessary to perform highly performant batch inference using a cluster.  In this notebook, we'll examine distributed training and distributed inference with TensorFlow in Amazon SageMaker. \n",
    "\n",
    "The model used for this notebook is a basic Convolutional Neural Network (CNN) based on [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py).  We'll train the CNN to classify images using the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), a well-known computer vision dataset. It consists of 60,000 32x32 images belonging to 10 different classes (6,000 images per class). Here is a graphic of the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "We'll begin with some necessary imports, and get an Amazon SageMaker session to help perform certain tasks, as well as an IAM role with the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-tf-horovod-inference'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run a script that fetches the dataset and converts it to the TFRecord format, which provides several conveniences for training models in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_cifar10_tfrecords.py --data-dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Amazon SageMaker hosted training on a cluster separate from this notebook instance, training data must be stored in Amazon S3, so we'll upload the data to S3 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10-tf')\n",
    "display(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll perform some setup that will be common to all of the training jobs in this notebook.  During training, it will helpful to review metric graphs about the training job such as validation accuracy.  If we provide a set of metric definitions, Amazon SageMaker will be able to get the metrics directly from the training job logs, and send them to CloudWatch Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metric_definitions = [\n",
    "    {'Name': 'train:loss', 'Regex': '.*loss: ([0-9\\\\.]+) - acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'train:accuracy', 'Regex': '.*loss: [0-9\\\\.]+ - acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:accuracy', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:loss', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'sec/steps', 'Regex': '.* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: [0-9\\\\.]+'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)  Hosted Training on a single machine\n",
    "\n",
    "Amazon SageMaker provides a variety of model training options:  besides training on a single machine, training can be done on a cluster of multiple machines using either parameter servers or Ring-AllReduce with Horovod.  We'll begin by training a model on a single machine.  This will be followed by distributed training on multiple machines to allow comparison; however if you prefer you may skip ahead to the **Distributed training** section of this notebook.\n",
    "\n",
    "Initially we'll set up an Amazon SageMaker TensorFlow Estimator object with the details of the training job, such as type and number of instances, hyperparameters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "single_machine_instance_type = 'ml.p3.2xlarge'\n",
    "hyperparameters = {'epochs': 60, 'batch-size' : 256}\n",
    "\n",
    "estimator_single = TensorFlow(base_job_name='cifar10-tf',\n",
    "                       entry_point='train.py',\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type=single_machine_instance_type,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'file'}],\n",
    "                       metric_definitions=training_metric_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `fit` method of the Estimator object to start training.  During training, you can view the metrics we set up above by going to the Amazon SageMaker console, clicking the **Training jobs** link in the left panel, clicking the job name, then scrolling down to the **Monitor** section to view the metric graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {'train' : inputs+'/train', 'validation' : inputs+'/validation', 'eval' : inputs+'/eval'}\n",
    "estimator_single.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it makes sense to perform training on a single machine.  For large datasets, however, it may be necessary to perform distributed training on a cluster of multiple machines.  In fact, it may be not only faster but cheaper to do distributed training on several machines rather than one machine.  Fortunately, Amazon SageMaker makes it easy to run distributed training without having to manage cluster setup and tear down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Horovod\n",
    "\n",
    "Horovod is an open source distributed training framework for TensorFlow, Keras, PyTorch, and MXNet. It is an alternative to the more \"traditional\" parameter server method of performing distributed training.  In Amazon SageMaker, Horovod is only available with TensorFlow version 1.12 or newer. Only a few lines of code are necessary to use Horovod for distributed training of a Keras model defined by the tf.keras API.  For details, see the `train.py` script included with this notebook; the changes primarily relate to:\n",
    "\n",
    "- importing Horovod.\n",
    "- initializing Horovod.\n",
    "- configuring GPU options and setting a Keras/tf.session with those options.\n",
    "\n",
    "Once we have a training script, the next step is to set up an Amazon SageMaker TensorFlow Estimator object with the details of the training job.  It is very similar to an Estimator for training on a single machine, except we specify a `distributions` parameter describing Horovod attributes such as the number of process per host, which is set here to the number of GPUs per machine.  Beyond these few simple parameters and the few lines of code in the training script, there is nothing else you need to do to use distributed training with Horovod; Amazon SageMaker handles the heavy lifting for you and manages the underlying cluster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hvd_instance_type = 'ml.p3.8xlarge'\n",
    "hvd_processes_per_host = 4\n",
    "hvd_instance_count = 2\n",
    "\n",
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': hvd_processes_per_host\n",
    "                        }\n",
    "                }\n",
    "hyperparameters = {'epochs': 60, 'batch-size' : 256}\n",
    "\n",
    "estimator_dist = TensorFlow(base_job_name='dist-cifar10-tf',\n",
    "                       entry_point='train.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=hvd_instance_count, \n",
    "                       train_instance_type=hvd_instance_type,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n",
    "                       metric_definitions=training_metric_definitions,\n",
    "                       distributions=distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {'train' : inputs+'/train', 'validation' : inputs+'/validation', 'eval' : inputs+'/eval'}\n",
    "estimator_dist.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment with Amazon Elastic Inference\n",
    "\n",
    "Amazon SageMaker provides both real time inference and batch inference.  Although we will focus on batch inference below, let's start with a quick overview of setting up an Amazon SageMaker hosted endpoint for real time inference with TensorFlow Serving and image data.  The processes for setting up hosted endpoints and Batch Transform jobs have significant differences.  Additionally, we will discuss why and how to use Amazon Elastic Inference with the hosted endpoint.\n",
    "\n",
    "### Deploying the Model\n",
    "\n",
    "When considering the overall cost of a machine learning workload, inference often is the largest part, up to 90% of the total.  If a GPU instance type is used for real time inference, it typically is not fully utilized because, unlike training, real time inference does not involve continuously inputting large batches of data to the model.  Elastic Inference provides GPU acceleration suited for inference, allowing you to add inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance.\n",
    "\n",
    "The `deploy` method of the Estimator object creates an endpoint which serves prediction requests in near real time.  To utilize Elastic Inference with the SageMaker TensorFlow Serving container, simply provide an `accelerator_type` parameter, which determines the type of accelerator that is attached to your endpoint. Refer to the **Inference Acceleration** section of the [instance types chart](https://aws.amazon.com/sagemaker/pricing/instance-types) for a listing of the supported types of accelerators. \n",
    "\n",
    "Here we'll use a general purpose CPU compute instance type along with an Elastic Inference accelerator:  together they are much cheaper than the smallest P3 GPU instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator_dist.deploy(initial_instance_count=1,\n",
    "                                  instance_type='ml.m5.xlarge',\n",
    "                                  accelerator_type='ml.eia1.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Real time inference\n",
    "  \n",
    "Now that we have a Predictor object wrapping a real time Amazon SageMaker hosted enpoint, we'll define the label names and look at a sample of 10 images, one from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "images = []\n",
    "for entry in os.scandir('sample-img'):\n",
    "    if entry.is_file() and entry.name.endswith(\"png\"):\n",
    "        images.append('sample-img/' + entry.name)\n",
    "\n",
    "for image in images:\n",
    "    display(Image(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll set up the Predictor object created by the `deploy` method call above.  The TensorFlow Serving container in Amazon SageMaker uses the REST API, which requires requests in a specific JSON format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "# TensorFlow Serving's request and response format is JSON \n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "def get_prediction(file_path):\n",
    "    \n",
    "    image = PIL.Image.open(file_path)\n",
    "    to_numpy_list = np.asarray(image).astype(float)\n",
    "    instance = np.expand_dims(to_numpy_list, axis=0)\n",
    "    data = {'instances': instance.tolist()}\n",
    "    \n",
    "    return labels[np.argmax(predictor.predict(data)['predictions'], axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [get_prediction(image) for image in images]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Batch Transform with Inference Pipelines\n",
    "\n",
    "If a use case does not require individual predictions in near real-time, an Amazon SageMaker Batch Transform job is likely a better alternative. Although hosted endpoints also can be used for pseudo-batch prediction, the process is more involved than using the alternative Batch Transform, which is designed for large-scale, asynchronous batch inference.\n",
    "\n",
    "A typical problem in working with batch inference is how to convert data into tensors that can be input to the model.  For example, image data in .png or .jpg format cannot be directly input to a model, but rather must be converted first.  Batch Transform provides facilities for doing this efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container for transforming image input\n",
    "\n",
    "As mentioned above, the TensorFlow Serving container in Amazon SageMaker uses the REST API to serve prediction requests. This requires the input data to be converted to JSON format.  One way to do this is to create a container to do the conversion, then create an overall Amazon SageMaker model that links the conversion container to the TensorFlow Serving container with the model.\n",
    "\n",
    "In the next step, we'll create a container to transform payloads in .png image format into JSON objects that can be forwarded to the TensorFlow Serving container. To do this, we've created a simple Python Flask app that does the transformation, the code for this container is available in the `./image-transformer-container/` directory. First, we'll build a Docker image for the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t image-transformer ./image-transformer-container/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push container to ECR\n",
    "\n",
    "Next, we'll push the Docker image to an ECR repository in your account. In order to push the container to ECR, the execution role attached to this notebook should have permissions to create a repository, set a repository policy, and upload a Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'image-transformer'\n",
    "tag = ':latest'\n",
    "transformer_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "\n",
    "# docker login\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "# create ecr repository\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "# attach policy allowing sagemaker to pull this image\n",
    "!aws ecr set-repository-policy --repository-name $ecr_repository --policy-text \"$( cat ./image-transformer-container/ecr_policy.json )\"\n",
    "\n",
    "!docker tag {ecr_repository + tag} $transformer_repository_uri\n",
    "!docker push $transformer_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Model with an Inference Pipeline\n",
    "\n",
    "Now that we have two separate containers for transforming the data and serving predictions, we'll create an Amazon SageMaker Model with the two containers chained together (image transformer -> TensorFlow Serving).  The Model conveniently packages together the functionality required for an Inference Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "from time import gmtime, strftime\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "model_name = \"image-to-tfserving-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "\n",
    "transform_container = {\n",
    "    \"Image\": transformer_repository_uri\n",
    "}\n",
    "\n",
    "estimator = estimator_dist\n",
    "tf_serving_model = Model(model_data=estimator.model_data,\n",
    "                         role=sagemaker.get_execution_role(),\n",
    "                         image=estimator.image_name,\n",
    "                         framework_version=estimator.framework_version,\n",
    "                         sagemaker_session=estimator.sagemaker_session)\n",
    "\n",
    "batch_instance_type = 'ml.p3.8xlarge'\n",
    "tf_serving_container = tf_serving_model.prepare_container_def(batch_instance_type)\n",
    "\n",
    "model_params = {\n",
    "    \"ModelName\": model_name,\n",
    "    \"Containers\": [\n",
    "        transform_container,\n",
    "        tf_serving_container\n",
    "    ],\n",
    "    \"ExecutionRoleArn\": sagemaker.get_execution_role()\n",
    "}\n",
    "\n",
    "client.create_model(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Batch Transform job\n",
    "\n",
    "Next, we'll run a Batch Transform job. More specifically, we'll perform distributed inference on a cluster of two instances.  As an additional optimization, we'll set the `max_concurrent_transforms` parameter of the Transformer object, which controls the maximum number of parallel requests that can be sent to each instance in a transform job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = 's3://sagemaker-sample-data-{}/tensorflow/cifar10/images/png'.format(sagemaker_session.boto_region_name)\n",
    "output_data_path = 's3://{}/{}/{}'.format(bucket, prefix, 'batch-predictions')\n"",
    "output_data_path = 's3://{}/{}/{}'.format(bucket, prefix, 'batch-predictions')\n",
    "batch_instance_type = 'ml.p3.2xlarge'\n",
    "batch_instance_count = 2\n",
    "concurrency = 100\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name = model_name,\n",
    "    instance_count = batch_instance_count,\n",
    "    instance_type = batch_instance_type,\n",
    "    max_concurrent_transforms = concurrency,\n",
    "    strategy = 'MultiRecord',\n",
    "    output_path = output_data_path,\n",
    "    assemble_with= 'Line',\n",
    "    base_transform_job_name='cifar-10-image-transform',\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "transformer.transform(data = input_data_path, content_type = 'application/x-image')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Batch Transform output\n",
    "\n",
    "Finally, we can inspect the output files of our Batch Transform job to see the predictions.  First we'll download the prediction files locally, then extract the predictions from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --quiet --recursive $transformer.output_path ./batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for entry in os.scandir('batch_predictions'):\n",
    "    try:\n",
    "        if entry.is_file() and entry.name.endswith(\"out\"):\n",
    "            with open(entry, 'r') as f:\n",
    "                jstr = json.load(f)\n",
    "                results = [float('%.3f'%(item)) for sublist in jstr['predictions'] for item in sublist]\n",
    "                class_index = np.argmax(np.array(results))\n",
    "                predicted_label = labels[class_index]\n",
    "                predicted.append(predicted_label)\n",
    "                actual_label = re.search('([a-zA-Z]+).png.out', entry.name).group(1)\n",
    "                actual.append(actual_label)\n",
    "                is_correct = (predicted_label in entry.name) or False\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the accuracy of the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Out of {} total images, accurate predictions were returned for {}'.format(total, correct))\n",
    "accuracy = correct / total\n",
    "print('Accuracy is {:.1%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy from the batch transform job on 10000 test images never seen during training is fairly close to the accuracy achieved during training on the validation set.  This is an indication that the model is not overfitting and should generalize fairly well to other unseen data. \n",
    "\n",
    "Next we'll plot a confusion matrix, which is a tool for visualizing the performance of a multiclass model. It has entries for all possible combinations of correct and incorrect predictions, and shows how often each one was made by our model. Ours will be row-normalized: each row sums to one, so that entries along the diagonal correspond to recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix = pd.crosstab(pd.Series(actual), pd.Series(predicted), rownames=['Actuals'], colnames=['Predictions'], normalize='index')\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model had 100% accuracy, and therefore 100% recall in every class, then all of the predictions would fall along the diagonal of the confusion matrix.  Here our model definitely is not 100% accurate, but manages to achieve good recall for most of the classes, though it performs worse for some classes, such as cats.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Although we did not demonstrate them in this notebook, Amazon SageMaker provides additional ways to make distributed training more efficient for very large datasets:\n",
    "- **VPC training**:  performing Horovod training inside a VPC improves the network latency between nodes, leading to higher performance and stability of Horovod training jobs.\n",
    "- **Pipe Mode**:  using [Pipe Mode](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-inputdataconfig) reduces startup and training times.  Pipe Mode streams training data from S3 as a Linux FIFO directly to the algorithm, without saving to disk.  For a small dataset such as CIFAR-10, Pipe Mode does not provide any advantage, but for very large datasets where training is I/O bound rather than CPU/GPU bound, Pipe Mode can substantially reduce startup and training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "To avoid incurring charges due to a stray endpoint, delete the Amazon SageMaker endpoint if you no longer need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
