{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing TensorFlow Eager Execution with Amazon SageMaker’s Script Mode \n",
    "\n",
    "Starting with TensorFlow version 1.11, you can use SageMaker's prebuilt TensorFlow containers with TensorFlow training scripts similar to those you would use outside SageMaker. This feature is named Script Mode.\n",
    "\n",
    "In this notebook, we will use Script Mode in conjunction with TensorFlow's Eager Execution mode, which will become the default execution mode of TensorFlow 2 onwards.  Eager execution is an imperative interface where operations are executed immediately, rather than building a static computational graph. Advantages of Eager Execution include a more intuitive interface with natural Python control flow and less boilerplate, easier debugging, and support for dynamic models and almost all of the available TensorFlow operations. It also features close integration with tf.keras to make rapid prototyping even easier.  \n",
    "\n",
    "To demonstrate how Eager Execution works with Script Mode, this notebook focuses on presenting a relatively complete workflow. The workflow includes local and SageMaker hosted training, as well as local and SageMaker hosted inference, in the context of a straightforward regression task.  This task involves predicting house prices based on the well-known Boston Housing dataset. More specifically, this public dataset contains 13 features regarding housing stock of towns in the Boston area, including features such as average number of rooms, accessibility to radial highways, adjacency to the Charles River, etc.  \n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data, and direct TensorFlow to use Eager Execution mode rather than the default graph mode of TensorFlow 1.x.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "Next, we'll import the dataset. The dataset itself is small and relatively issue-free. For example, there are no missing values, a common problem for many other datasets. Accordingly, preprocessing just involves normalizing the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (404, 13) 2.6029783389231392e-15 0.9999999879626582\n",
      "y train (404,) 22.395049504950492 9.199035423364862\n",
      "x test (102, 13) 0.020826991529340172 0.9836083314719052\n",
      "y test (102,) 23.07843137254902 9.123806690181466\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# normalization of dataset\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "\n",
    "x_train = (x_train - mean) / (std + 1e-8)\n",
    "x_test = (x_test - mean) / (std + 1e-8)\n",
    "\n",
    "print('x train', x_train.shape, x_train.mean(), x_train.std())\n",
    "print('y train', y_train.shape, y_train.mean(), y_train.std())\n",
    "print('x test', x_test.shape, x_test.mean(), x_test.std())\n",
    "print('y test', y_test.shape, y_test.mean(), y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is saved as Numpy files prior to both local mode training and hosted training in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Local mode training\n",
    "\n",
    "Amazon SageMaker’s Local Mode training feature is a convenient way to make sure your code is working as expected before moving on to full scale, hosted training. To train in Local Mode, it is necessary to have docker-compose or nvidia-docker-compose (for GPU) installed in the notebook instance. Running following script will install docker-compose or nvidia-docker-compose and configure the notebook environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a TensorFlow Estimator for Local Mode training. One of the key parameters for an Estimator is the `train_instance_type`, which is the kind of hardware on which training will run. In the case of Local Mode, we simply set this parameter to `local` to invoke Local Mode training on the CPU, or to `local_gpu` if the instance has a GPU. Other parameters of note are the algorithm’s hyperparameters, which are passed in as a dictionary, and a Boolean parameter indicating that we are using Script Mode. \n",
    "\n",
    "Recall that we are using Local Mode here mainly to make sure our code is working. Accordingly, instead of performing a full cycle of training with many epochs (passes over the full dataset), we'll train only for a small number of epochs to confirm the code is working properly and avoid wasting training time unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'local'\n",
    "hyperparameters = {'epochs': 10, 'batch_size': 128}\n",
    "local_estimator = TensorFlow(entry_point='train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-eager-scriptmode-bostonhousing',\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpqg6ggkb2_algo-1-435nr_1 ... \n",
      "\u001b[1BAttaching to tmpqg6ggkb2_algo-1-435nr_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m 2019-02-04 19:13:03,283 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m 2019-02-04 19:13:03,289 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m 2019-02-04 19:13:03,439 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m 2019-02-04 19:13:03,456 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m 2019-02-04 19:13:03,469 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"current_host\": \"algo-1-435nr\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"algo-1-435nr\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"epochs\": 10,\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"batch_size\": 128,\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"model_dir\": \"/opt/ml/model\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"job_name\": \"tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"master_hostname\": \"algo-1-435nr\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"current_host\": \"algo-1-435nr\",\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m             \"algo-1-435nr\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_HOSTS=[\"algo-1-435nr\"]\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_HPS={\"batch_size\":128,\"epochs\":10,\"model_dir\":\"/opt/ml/model\"}\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-435nr\",\"hosts\":[\"algo-1-435nr\"]}\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_CURRENT_HOST=algo-1-435nr\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-435nr\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-435nr\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":10,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459\",\"log_level\":20,\"master_hostname\":\"algo-1-435nr\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-435nr\",\"hosts\":[\"algo-1-435nr\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"10\",\"--model_dir\",\"/opt/ml/model\"]\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_HP_EPOCHS=10\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_HP_BATCH_SIZE=128\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m SM_HP_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m /usr/bin/python train.py --batch_size 128 --epochs 10 --model_dir /opt/ml/model\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m \n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m x train (404, 13) y train (404,)\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m x test (102, 13) y test (102,)\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m /cpu:0\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m batch_size = 128, epochs = 10\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 1/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 199.4480 - val_loss: 55.5963\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 2/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 44.5407 - val_loss: 37.2407\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 3/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 34.6283 - val_loss: 34.1923\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 4/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.7356 - val_loss: 29.2304\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 5/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 35.9682 - val_loss: 49.8270\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 6/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 26.6935 - val_loss: 27.4554\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 7/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 20.8584 - val_loss: 23.3701\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 8/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.3810 - val_loss: 24.9047\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 9/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.7848 - val_loss: 23.1638\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 13.8906 - val_loss: 20.5614\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m Test MSE : 20.56144142150879\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m WARNING:tensorflow:Export includes no default signature!\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m WARNING:tensorflow:Export includes no default signature!\n",
      "\u001b[36malgo-1-435nr_1  |\u001b[0m 2019-02-04 19:13:08,953 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpqg6ggkb2_algo-1-435nr_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "inputs = {'train': f'file://{train_dir}',\n",
    "          'test': f'file://{test_dir}'}\n",
    "\n",
    "local_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've confirmed that our code is working, we have a model checkpoint saved in S3 that we can retrieve and load.  We can then make predictions and compare them with the test set as a further sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 24.9 KiB/24.9 KiB (104.4 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-12-59-459/model.tar.gz to local_model/model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {local_estimator.model_data} ./local_model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549307588/\r\n",
      "1549307588/saved_model.pb\r\n",
      "1549307588/variables/\r\n",
      "1549307588/variables/variables.index\r\n",
      "1549307588/variables/variables.data-00000-of-00001\r\n",
      "1549307588/variables/checkpoint\r\n",
      "1549307588/assets/\r\n",
      "1549307588/assets/saved_model.json\r\n",
      "checkpoint\r\n",
      "weights.ckpt.index\r\n",
      "weights.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./local_model/model.tar.gz -C ./local_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from local_model/weights.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from local_model/weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.eager.python import tfe\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(13,))\n",
    "    hidden_1 = tf.keras.layers.Dense(13, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(6, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "device = '/cpu:0' \n",
    "\n",
    "with tf.device(device):    \n",
    "    local_model = get_model()\n",
    "    saver = tfe.Saver(local_model.variables)\n",
    "    saver.restore('local_model/weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the saved model restored, we can now generate predictions and compare them to the actual housing prices in the test set. The values are in units of $1000s. In case you're wondering why the actual values seem relatively low compared to today's big city housing prices:  the paper referencing the dataset was originally published in 1978. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[10.3 18.3 21.3 30.8 23.9 21.8 31.7 23.4 16.2 22.2]\n",
      "target values: \t[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):   \n",
    "    local_predictions = local_model.predict(x_test)\n",
    "    \n",
    "print('predictions: \\t{}'.format(local_predictions[:10].flatten().round(decimals=1)))\n",
    "print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted training\n",
    "\n",
    "Now that we've confirmed our code is working locally, we can move on to use SageMaker's hosted training functionality. Hosted training is preferred to for doing actual training, especially large-scale, distributed training.  Before starting hosted training, the data must be uploaded to S3. We'll do that now, and confirm the upload was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'tf-eager-scriptmode-bostonhousing'\n",
    "\n",
    "traindata_s3_prefix = '{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '{}/data/test'.format(s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing/data/train', 'test': 's3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing/data/test'}\n"
     ]
    }
   ],
   "source": [
    "train_s3 = sagemaker.Session().upload_data(path='./data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sagemaker.Session().upload_data(path='./data/test/', key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to set up an Estimator object for hosted training. It is similar to the Local Mode Estimator, except the `train_instance_type` has been set to a ML instance type instead of `local` for Local Mode. Also, since we know our code is working now, we train for a larger number of epochs.\n",
    "\n",
    "With these two changes, we simply call `fit` to start the actual hosted training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = 'ml.c4.xlarge'\n",
    "hyperparameters = {'epochs': 30, 'batch_size': 128}\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-eager-scriptmode-bostonhousing',\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-04 19:13:52 Starting - Starting the training job...\n",
      "2019-02-04 19:13:53 Starting - Launching requested ML instances......\n",
      "2019-02-04 19:14:57 Starting - Preparing the instances for training...\n",
      "2019-02-04 19:15:41 Downloading - Downloading input data...\n",
      "2019-02-04 19:16:19 Training - Training image download completed. Training in progress.\n",
      "2019-02-04 19:16:19 Uploading - Uploading generated training model\n",
      "2019-02-04 19:16:19 Completed - Training job completed\n",
      "\n",
      "\u001b[31m2019-02-04 19:16:05,676 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[31m2019-02-04 19:16:05,682 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-02-04 19:16:05,930 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-02-04 19:16:05,945 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-02-04 19:16:05,956 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 30\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HPS={\"batch_size\":128,\"epochs\":30,\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":30,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"ethwe\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"30\",\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[31mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=30\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python train.py --batch_size 128 --epochs 30 --model_dir /opt/ml/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mx train (404, 13) y train (404,)\u001b[0m\n",
      "\u001b[31mx test (102, 13) y test (102,)\u001b[0m\n",
      "\u001b[31m/cpu:0\u001b[0m\n",
      "\u001b[31mbatch_size = 128, epochs = 30\u001b[0m\n",
      "\u001b[31mEpoch 1/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 425.5030#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 22ms/step - loss: 199.4480 - val_loss: 55.5963\u001b[0m\n",
      "\u001b[31mEpoch 2/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 20.7183#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 44.5407 - val_loss: 37.2407\u001b[0m\n",
      "\u001b[31mEpoch 3/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 59.2575#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 34.6283 - val_loss: 34.1923\u001b[0m\n",
      "\u001b[31mEpoch 4/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 18.8119#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 24.7356 - val_loss: 29.2304\u001b[0m\n",
      "\u001b[31mEpoch 5/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 24.7812#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 35.9682 - val_loss: 49.8270\u001b[0m\n",
      "\u001b[31mEpoch 6/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 26.7028#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 26.6935 - val_loss: 27.4554\u001b[0m\n",
      "\u001b[31mEpoch 7/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 31.7002#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 20.8584 - val_loss: 23.3701\u001b[0m\n",
      "\u001b[31mEpoch 8/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 20.4839#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 16.3810 - val_loss: 24.9047\u001b[0m\n",
      "\u001b[31mEpoch 9/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 21.3874#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 13.7848 - val_loss: 23.1638\u001b[0m\n",
      "\u001b[31mEpoch 10/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 20.4363#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 13.8906 - val_loss: 20.5614\u001b[0m\n",
      "\u001b[31mEpoch 11/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 3.2722#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 17.0215 - val_loss: 36.9249\u001b[0m\n",
      "\u001b[31mEpoch 12/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 27.0239#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 20.6242 - val_loss: 35.4510\u001b[0m\n",
      "\u001b[31mEpoch 13/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 28.0116#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 20.6734 - val_loss: 22.9207\u001b[0m\n",
      "\u001b[31mEpoch 14/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 3.5845#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 11.1318 - val_loss: 25.3681\u001b[0m\n",
      "\u001b[31mEpoch 15/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 3.1606#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 12.3399 - val_loss: 20.3143\u001b[0m\n",
      "\u001b[31mEpoch 16/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 20.3922#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 21.9181 - val_loss: 24.9260\u001b[0m\n",
      "\u001b[31mEpoch 17/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 7.4762#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 15.8677 - val_loss: 33.1074\u001b[0m\n",
      "\u001b[31mEpoch 18/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 15.7872#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 12.6996 - val_loss: 18.3294\u001b[0m\n",
      "\u001b[31mEpoch 19/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 5.5650#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 13.5298 - val_loss: 21.8488\u001b[0m\n",
      "\u001b[31mEpoch 20/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 20.0818#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 12.2671 - val_loss: 19.9502\u001b[0m\n",
      "\u001b[31mEpoch 21/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 12.6693#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 11.7558 - val_loss: 23.1451\u001b[0m\n",
      "\u001b[31mEpoch 22/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 3.2922#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 9.6438 - val_loss: 19.7969\u001b[0m\n",
      "\u001b[31mEpoch 23/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 13.6852#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 11.3315 - val_loss: 22.7612\u001b[0m\n",
      "\u001b[31mEpoch 24/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 8.5784#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 8.4515 - val_loss: 30.9479\u001b[0m\n",
      "\u001b[31mEpoch 25/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 15.7818#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 11.5082 - val_loss: 19.0368\u001b[0m\n",
      "\u001b[31mEpoch 26/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 7.7624#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 7.4684 - val_loss: 18.4409\u001b[0m\n",
      "\u001b[31mEpoch 27/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 7.2318#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 7.2745 - val_loss: 18.7956\u001b[0m\n",
      "\u001b[31mEpoch 28/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 7.9631#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 7.6949 - val_loss: 29.3389\u001b[0m\n",
      "\u001b[31mEpoch 29/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 5.7434#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 8.6218 - val_loss: 17.2008\u001b[0m\n",
      "\u001b[31mEpoch 30/30\u001b[0m\n",
      "\u001b[31m#0151/4 [======>.......................] - ETA: 0s - loss: 9.8884#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 4ms/step - loss: 8.5679 - val_loss: 17.5906\u001b[0m\n",
      "\u001b[31mTest MSE : 17.5905818939209\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[31m2019-02-04 19:16:12,227 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 38\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Local Mode training, hosted training produces a model checkpoint saved in S3 that we can retrieve and load. We can then make predictions and compare them with the test set.  This also demonstrates the modularity of SageMaker: having trained the model in SageMaker, you can now take the model out of SageMaker and run it anywhere else.  Alternatively, you can deploy the model using SageMaker's hosted endpoints functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 25.7 KiB/25.7 KiB (412.9 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-894087409521/tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164/output/model.tar.gz to model/model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.ckpt.index\r\n",
      "weights.ckpt.data-00000-of-00001\r\n",
      "checkpoint\r\n",
      "1549307771/\r\n",
      "1549307771/variables/\r\n",
      "1549307771/variables/checkpoint\r\n",
      "1549307771/variables/variables.index\r\n",
      "1549307771/variables/variables.data-00000-of-00001\r\n",
      "1549307771/saved_model.pb\r\n",
      "1549307771/assets/\r\n",
      "1549307771/assets/saved_model.json\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/weights.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "device = '/cpu:0' \n",
    "\n",
    "with tf.device(device):    \n",
    "    model = get_model()\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.restore('model/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[ 9.9 19.8 21.9 27.9 24.3 21.3 33.6 24.5 19.4 21.1]\n",
      "target values: \t[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):   \n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "print('predictions: \\t{}'.format(predictions[:10].flatten().round(decimals=1)))\n",
    "print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted endpoint\n",
    "\n",
    "After multiple sanity checks, we're confident that our model is performing as expected. If we wish to deploy the model to production, a convenient option is to use a SageMaker hosted endpoint. The endpoint will retrieve the TensorFlow SavedModel created during training and deploy it within a TensorFlow Serving container. This all can be accomplished with one line of code, an invocation of the Estimator's deploy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164\n",
      "INFO:sagemaker:Creating endpoint with name tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last sanity check, we can compare the predictions generated by the endpoint with those generated locally by the model checkpoint we retrieved from hosted training in SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[ 9.9 19.8 21.9 27.9 24.3 21.3 33.6 24.5 19.4 21.1]\n",
      "target values: \t[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2]\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(x_test[:10])['predictions'] \n",
    "flat_list = [float('%.1f'%(item)) for sublist in results for item in sublist]\n",
    "print('predictions: \\t{}'.format(np.array(flat_list)))\n",
    "print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're finished with your review of this notebook, you can delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: tf-eager-scriptmode-bostonhousing-2019-02-04-19-13-51-164\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
