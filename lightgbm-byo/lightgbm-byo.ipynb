{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYO Container Example:  lightGBM\n",
    "\n",
    "In this notebook we'll examine how to BYO container in Amazon SageMaker.  This is an option for algorithms and frameworks not directly supported in Amazon SageMaker as either (1) built-in algorithms, or (2) prebuilt Amazon SageMaker containers (such as the ones for TensorFlow, PyTorch, Apache MXNet, Scikit-learn, and XGBoost).  As an example, we'll containerize the popular lightGBM gradient boosting framework, which is not supported off-the-shelf in Amazon SageMaker, and apply it to a public dataset from UCI's Machine Learning Repository.  The dataset, which relates to predicting purchase intent by online shoppers, is at https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset.  \n",
    "\n",
    "Besides BYO container, we'll also employ the following features:  SageMaker Processing for data preprocessing, SageMaker hosted training, SageMaker Processing for model evaluation/batch scoring, and endpoints for real time inference.  More specifically, we'll perform the following steps:\n",
    "\n",
    "- Obtain the dataset.\n",
    "- Build a Docker image for lightGBM to be run as a container in SageMaker Processing.\n",
    "- Preprocess the data with that image in SageMaker Processing.\n",
    "- Build a separate Docker image for lightGBM for training models with SageMaker hosted training.\n",
    "- Train a lightGBM model with that separate Docker image in SageMaker hosted training.\n",
    "- Evaluate the model / do batch scoring in SageMaker Processing with the same container used for preprocessing.\n",
    "- Deploy the model to a real time SageMaker endpoint using Ezsmdeploy, a Python package that provides many conveniences and automation.\n",
    "\n",
    "**PREREQUISITES:**  Be sure to run this notebook on an instance or machine that supports Docker, with AWS IAM permissions to Amazon S3 and Amazon SageMaker (full access to both is fine for learning purposes).  In the notebook below we'll also add access to Amazon ECR.\n",
    "\n",
    "**DO NOT SELECT \"RUN ALL\" CELLS**\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll begin with updating version of Sagemaker Python SDK and some imports that will be useful throughout the notebook, and set up some objects and variables we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::553020858742:role/service-role/AirflowSageMakerExecutionRole\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "session = sagemaker.Session()\n",
    "s3_output = session.default_bucket()\n",
    "s3_prefix = 'lightGBM-BYO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's add access via AWS IAM to Amazon ECR, a fully-managed Docker container registry that makes it easy to store, manage, and deploy Docker container images.  Later in this notebook we'll create Docker repositories in ECR and push Docker images to them.\n",
    "\n",
    "To do this, perform the following steps:\n",
    "\n",
    "- In a separate browser tab, open the IAM console:  https://console.aws.amazon.com/iam\n",
    "- In the left panel/tray, click **Roles**.\n",
    "- Click on the name of your role as printed above (it will be the characters after the right-most \"/\" character).\n",
    "- Click on the **Attach policies** button.\n",
    "- In the search box, type **ec2containerregistry**; you should now see a list of policies that include the substring \"AmazonEC2ContainerRegistry\".\n",
    "- Click the box next to **AmazonEC2ContainerRegistryFullAccess**.\n",
    "- Click **Attach policy**.\n",
    "\n",
    "Continue with the rest of the notebook, your role will be updated almost instantaneously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain dataset\n",
    "\n",
    "Next we'll download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-30 15:51:48--  https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1072063 (1.0M) [application/x-httpd-php]\n",
      "Saving to: ‘./raw/online_shoppers_intention.csv.1’\n",
      "\n",
      "online_shoppers_int 100%[===================>]   1.02M  2.60MB/s    in 0.4s    \n",
      "\n",
      "2020-09-30 15:51:49 (2.60 MB/s) - ‘./raw/online_shoppers_intention.csv.1’ saved [1072063/1072063]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p raw\n",
    "!wget -P ./raw https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the data briefly now, just to confirm it was properly downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.20       0.20         0.0         0.0   Feb                 1   \n",
       "1         0.00       0.10         0.0         0.0   Feb                 2   \n",
       "2         0.20       0.20         0.0         0.0   Feb                 4   \n",
       "3         0.05       0.14         0.0         0.0   Feb                 3   \n",
       "4         0.02       0.05         0.0         0.0   Feb                 3   \n",
       "\n",
       "   Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0        1       1            1  Returning_Visitor    False    False  \n",
       "1        2       1            2  Returning_Visitor    False    False  \n",
       "2        1       9            3  Returning_Visitor    False    False  \n",
       "3        2       2            4  Returning_Visitor    False    False  \n",
       "4        3       1            4  Returning_Visitor     True    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./raw/online_shoppers_intention.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target we'd like to predict is the Revenue column, which is `True` if an online purchase transaction was completed.  As you might expect, a relatively small number of transactions are actually completed, resulting in a class imbalance we can handle various ways with lightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.countplot(df['Revenue'])\n",
    "plt.ylim(0,12000)\n",
    "plt.xlabel('Transactions Completed', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.text(x=-.175, y=11000 ,s='10,422', fontsize=16)\n",
    "plt.text(x=.875, y=2500, s='1908', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Exploratory Data Analysis (EDA) is not the focus of this example, we'll now move on to upload the raw data to S3 so it can be accessed by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/raw\n"
     ]
    }
   ],
   "source": [
    "rawdata_s3_prefix = '{}/raw'.format(s3_prefix)\n",
    "raw_s3 = session.upload_data(path='./raw/', key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Experiments setup\n",
    "\n",
    "SageMaker Experiments allows us to keep track of data preprocessing and model training; organize related models together; and log model configuration, parameters, and metrics to reproduce and iterate on previous models and compare models. We'll create a single experiment to keep track of the different approaches we'll use to train the model.\n",
    "\n",
    "Each approach or block of preprocessing or training code that we run can be an experiment trial. Later, we'll be able to compare different trials.  To start, we'll install the SageMaker Experiments SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a few parameters are required to create the SageMaker Experiments object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7fa5f4064208>,experiment_name='lightgbm-1601481133',description='Purchase intent prediction with lightGBM',tags=None,experiment_arn='arn:aws:sagemaker:us-east-1:553020858742:experiment/lightgbm-1601481133',response_metadata={'RequestId': '3668e113-bd87-47df-8fa9-1ee4a8af8c30', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3668e113-bd87-47df-8fa9-1ee4a8af8c30', 'content-type': 'application/x-amz-json-1.1', 'content-length': '91', 'date': 'Wed, 30 Sep 2020 15:52:14 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "import time\n",
    "\n",
    "lightgbm_experiment = Experiment.create(\n",
    "    experiment_name=f\"lightgbm-{int(time.time())}\", \n",
    "    description=\"Purchase intent prediction with lightGBM\", \n",
    "    sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "print(lightgbm_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker image for data preprocessing and model evaluation\n",
    "\n",
    "Before any further steps can be completed for SageMake Processing with lightGBM, we need to build a Docker image.  We'll build one image first, and use that same image for multiple purposes:\n",
    "\n",
    "- Preprocessing data; and\n",
    "- Evaluating the model (batch scoring).\n",
    "\n",
    "A separate, but very similar, Docker image will be used for training below.  \n",
    "\n",
    "To begin, we'll create a new directory for Docker-related files and write a Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docker-proc-evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Dockerfile can be used to build the container.  Of particular note are the following statements in the Dockerfile:\n",
    "- FROM statement:  this sets the parent image.  There are many choices, considerations include size (smaller may be better), \"up-to-dateness\", stability, and security.  The chosen image is based on a slim version of Debian 10 (\"Buster\").  \n",
    "- RUN statements:  used here primarily to install dependencies.  Only a few are required.  Note that libgomp1 is a library used by lightgbm, but is not included in this version of Debian.\n",
    "- ENTRYPOINT statement:  specifies the command used to run the scripts that will be included in the container by SageMaker.  In our case, they are ordinary Python 3 scripts so the command is simply `python3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-proc-evaluate/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-proc-evaluate/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "RUN apt -y update && apt install -y --no-install-recommends \\\n",
    "    libgomp1 \\\n",
    "    && apt clean    \n",
    "RUN pip3 install lightgbm numpy pandas scikit-learn \n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code builds the image using various Docker commands, creates an Amazon ECR repository, and pushes the image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/5 : FROM python:3.7-slim-buster\n",
      " ---> 4d4a9832278b\n",
      "Step 2/5 : RUN apt -y update && apt install -y --no-install-recommends     libgomp1 build-essential     && apt clean\n",
      " ---> Using cache\n",
      " ---> 954365d3ec9c\n",
      "Step 3/5 : RUN pip3 install lightgbm numpy pandas scikit-learn sagemaker-training\n",
      " ---> Running in 45ebe7298610\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.0.0-py2.py3-none-manylinux1_x86_64.whl (1.7 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting sagemaker-training\n",
      "  Downloading sagemaker_training-3.6.2.tar.gz (40 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.15.8-py2.py3-none-any.whl (129 kB)\n",
      "Collecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (from sagemaker-training) (20.2.3)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Collecting gevent\n",
      "  Downloading gevent-20.9.0-cp37-cp37m-manylinux2010_x86_64.whl (5.5 MB)\n",
      "Collecting inotify_simple==1.2.1\n",
      "  Downloading inotify_simple-1.2.1.tar.gz (7.9 kB)\n",
      "Collecting werkzeug>=0.15.5\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting paramiko>=2.4.2\n",
      "  Downloading paramiko-2.7.2-py2.py3-none-any.whl (206 kB)\n",
      "Collecting psutil>=5.6.7\n",
      "  Downloading psutil-5.7.2.tar.gz (460 kB)\n",
      "Collecting protobuf>=3.1\n",
      "  Downloading protobuf-3.13.0-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.19.0,>=1.18.8\n",
      "  Downloading botocore-1.18.8-py2.py3-none-any.whl (6.6 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.1.0-cp37-cp37m-manylinux2010_x86_64.whl (235 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from gevent->sagemaker-training) (50.3.0)\n",
      "Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
      "  Downloading greenlet-0.4.17-cp37-cp37m-manylinux1_x86_64.whl (45 kB)\n",
      "Collecting zope.event\n",
      "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
      "Collecting cryptography>=2.5\n",
      "  Downloading cryptography-3.1.1-cp35-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
      "Collecting urllib3<1.26,>=1.20; python_version != \"3.4\"\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting cffi>=1.1\n",
      "  Downloading cffi-1.14.3-cp37-cp37m-manylinux1_x86_64.whl (401 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Building wheels for collected packages: sagemaker-training, retrying, inotify-simple, psutil\n",
      "  Building wheel for sagemaker-training (setup.py): started\n",
      "  Building wheel for sagemaker-training (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-training: filename=sagemaker_training-3.6.2-cp37-cp37m-linux_x86_64.whl size=54763 sha256=724453fbdf27bf23bf2cfb744b36317696f919b6cdb855e41ed692e6432e1bb5\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/48/0b/b46b1798e095ef99ddaaea2637da0411272367842fbd41e1b6\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11429 sha256=c94163979da0c604a0ff73ebdd7a5d51f0765f48286b8433265ca7c0963a300c\n",
      "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
      "  Building wheel for inotify-simple (setup.py): started\n",
      "  Building wheel for inotify-simple (setup.py): finished with status 'done'\n",
      "  Created wheel for inotify-simple: filename=inotify_simple-1.2.1-py3-none-any.whl size=8203 sha256=10c8f92087562423de65cb3addb977d0642d96bb97f8d45bf192367f2ce9e0c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/ef/7e/4a/bfeb3216a60ab5e077958f5a1e980cc3de9663155cfb31c660\n",
      "  Building wheel for psutil (setup.py): started\n",
      "  Building wheel for psutil (setup.py): finished with status 'done'\n",
      "  Created wheel for psutil: filename=psutil-5.7.2-cp37-cp37m-linux_x86_64.whl size=224310 sha256=5f7d235b4216030302e13bd6dd27f02d4e7e1f83196f153c48d41ed2e79d31e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/43/97/00701864a7bee6d9e1a52dd682537dcbf1d013d0e2e6f0c1f1\n",
      "Successfully built sagemaker-training retrying inotify-simple psutil\n",
      "Installing collected packages: numpy, scipy, joblib, threadpoolctl, scikit-learn, lightgbm, pytz, six, python-dateutil, pandas, jmespath, urllib3, botocore, s3transfer, boto3, retrying, zope.interface, greenlet, zope.event, gevent, inotify-simple, werkzeug, pycparser, cffi, bcrypt, cryptography, pynacl, paramiko, psutil, protobuf, sagemaker-training\n",
      "Successfully installed bcrypt-3.2.0 boto3-1.15.8 botocore-1.18.8 cffi-1.14.3 cryptography-3.1.1 gevent-20.9.0 greenlet-0.4.17 inotify-simple-1.2.1 jmespath-0.10.0 joblib-0.16.0 lightgbm-3.0.0 numpy-1.19.2 pandas-1.1.2 paramiko-2.7.2 protobuf-3.13.0 psutil-5.7.2 pycparser-2.20 pynacl-1.4.0 python-dateutil-2.8.1 pytz-2020.1 retrying-1.3.3 s3transfer-0.3.3 sagemaker-training-3.6.2 scikit-learn-0.23.2 scipy-1.5.2 six-1.15.0 threadpoolctl-2.1.0 urllib3-1.25.10 werkzeug-1.0.1 zope.event-4.5.0 zope.interface-5.1.0\n",
      "Removing intermediate container 45ebe7298610\n",
      " ---> 001904cb83a2\n",
      "Step 4/5 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in 2f758ba12c44\n",
      "Removing intermediate container 2f758ba12c44\n",
      " ---> d0fec4c01c39\n",
      "Step 5/5 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in e04c5a9cab28\n",
      "Removing intermediate container e04c5a9cab28\n",
      " ---> 7293957767db\n",
      "Successfully built 7293957767db\n",
      "Successfully tagged lightgbm-byo-proc-eval:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'lightgbm-byo-proc-eval' already exists in the registry with id '553020858742'\n",
      "The push refers to repository [553020858742.dkr.ecr.us-east-1.amazonaws.com/lightgbm-byo-proc-eval]\n",
      "\n",
      "\u001b[1Ba159bd3a: Preparing \n",
      "\u001b[1Be7a5df1b: Preparing \n",
      "\u001b[1Be5632dc8: Preparing \n",
      "\u001b[1B857805ec: Preparing \n",
      "\u001b[1B87503449: Preparing \n",
      "\u001b[1B6688d36c: Preparing \n",
      "\u001b[7Ba159bd3a: Pushed   421.8MB/412.8MBA\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2KPushing  331.6MB/412.8MB\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:5a148c836f76ace9c907949eabc85cd489e31da44a627bfd3c29e1a2915192bd size: 1795\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'lightgbm-byo-proc-eval'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker-proc-evaluate\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Docker image has all required dependencies, and enables you to run your own preprocessing, feature engineering, and model evaluation scripts all within the same container in a robust and repeatable way. \n",
    "\n",
    "To integrate the image with SageMaker, simply reference it in the SageMaker Python SDK's `ScriptProcessor` class, which lets you execute a command to run your own script inside a container based on this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=processing_repository_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data with SageMaker Processing\n",
    "\n",
    "Some preprocessing should be performed on this dataset before training.  For example, the data must be normalized, and split into train and test sets.  Below is a preprocessing script.  It is an ordinary Python script with very little specific to SageMaker.  To comply with SageMaker, the script must read the input data from a specified directory, and save the preprocessed data to certain directories so it can be automatically uploaded to S3 by SageMaker at the end of the job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    input_file = glob.glob('{}/*.csv'.format('/opt/ml/processing/input'))\n",
    "    print('\\nINPUT FILE: \\n{}\\n'.format(input_file))   \n",
    "    df = pd.read_csv(input_file[0])\n",
    "    \n",
    "    # minor preprocessing (drop some uninformative columns etc.)\n",
    "    print('Preprocessing the dataset . . . .')   \n",
    "    df_clean = df.drop(['Month','Browser','OperatingSystems','Region','TrafficType','Weekend'], axis=1)\n",
    "    visitor_encoded = pd.get_dummies(df_clean['VisitorType'], prefix='Visitor_Type', drop_first = True)\n",
    "    df_clean_merged = pd.concat([df_clean, visitor_encoded], axis=1).drop(['VisitorType'], axis=1)\n",
    "    X = df_clean_merged.drop('Revenue', axis=1)\n",
    "    y = df_clean_merged['Revenue']\n",
    "    \n",
    "    # split the preprocessed data with stratified sampling for class imbalance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=2, test_size=.2)\n",
    "\n",
    "    # save to container directory for uploading to S3\n",
    "    print('Saving the preprocessed dataset . . . .')   \n",
    "    train_data_output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "    np.save(train_data_output_path, X_train.to_numpy())\n",
    "    train_labels_output_path = os.path.join('/opt/ml/processing/train', 'y_train.npy')\n",
    "    np.save(train_labels_output_path, y_train.to_numpy())    \n",
    "    test_data_output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "    np.save(test_data_output_path, X_test.to_numpy())\n",
    "    test_labels_output_path = os.path.join('/opt/ml/processing/test', 'y_test.npy')\n",
    "    np.save(test_labels_output_path, y_test.to_numpy())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `ScriptProcessor` object created above can be used to run this `preprocessing.py` script. As mentioned above, the primary requirements are specifying input and output directories.  Here, there are two outputs because the transformed train and test data are sent to different folders in S3.  We also include an `experiment_config` parameter so this data preprocessing step can be tracked as part of a SageMaker Experiment and added to model lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name lightgbm-byo-process-30-15-55-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  lightgbm-byo-process-30-15-55-28\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightGBM-BYO/raw', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightgbm-byo-process-30-15-55-28/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".....................\n",
      "\u001b[34mINPUT FILE: \u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/input/online_shoppers_intention.csv']\n",
      "\u001b[0m\n",
      "\u001b[34mPreprocessing the dataset . . . .\u001b[0m\n",
      "\u001b[34mSaving the preprocessed dataset . . . .\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime \n",
    "\n",
    "processing_job_name = \"lightgbm-byo-process-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/data'.format(s3_output, s3_prefix)\n",
    "\n",
    "script_processor.run(code='preprocessing.py',\n",
    "                      job_name=processing_job_name,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_s3,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train',\n",
    "                                                destination='{}/train'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test',\n",
    "                                                destination='{}/test'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/test')],\n",
    "                      experiment_config={\n",
    "                            \"ExperimentName\": lightgbm_experiment.experiment_name,\n",
    "                            \"TrialComponentDisplayName\": \"Processing\",\n",
    "                      }\n",
    "                    )\n",
    "\n",
    "preprocessing_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job is complete, it is easy to look up the location of the output in S3.  The code below retrieves the S3 URLs of the locations of the transformed train and test data.  These will be used as inputs to futher jobs below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/train\n",
      "s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/test\n"
     ]
    }
   ],
   "source": [
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "        print(preprocessed_training_data)\n",
    "    if output['OutputName'] == 'test':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "        print(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can download the preprocessed test data for later use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.download_data(path='.', bucket=s3_output, key_prefix=s3_prefix+'/data/test/x_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the SageMaker Processing job, a SageMaker Experiments trial component was associated with it so we can include it when tracking model lineage.  We can inspect all of the information automatically logged by the Experiment Tracker during the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7fa5eee73710>,trial_component_name='lightgbm-byo-process-30-15-55-28-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:553020858742:experiment-trial-component/lightgbm-byo-process-30-15-55-28-aws-processing-job',display_name='Processing',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:553020858742:processing-job/lightgbm-byo-process-30-15-55-28',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='InProgress',message='Status: InProgress, exit message: null, failure reason: null'),creation_time=datetime.datetime(2020, 9, 30, 15, 55, 50, 170000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 9, 30, 15, 55, 50, 170000, tzinfo=tzlocal()),last_modified_by={},parameters={'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.xlarge', 'SageMaker.VolumeSizeInGB': 30.0},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='553020858742.dkr.ecr.us-east-1.amazonaws.com/lightgbm-byo-proc-eval:latest',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-553020858742/lightgbm-byo-process-30-15-55-28/input/code/preprocessing.py',media_type=None), 'input-1': TrialComponentArtifact(value='s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/raw',media_type=None)},output_artifacts={'test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/test',media_type=None), 'train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/train',media_type=None)},metrics=[],response_metadata={'RequestId': '8d313bb3-4fda-497c-8dfe-b0ca2c86a015', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '8d313bb3-4fda-497c-8dfe-b0ca2c86a015', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1293', 'date': 'Wed, 30 Sep 2020 15:59:10 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "for trial in lightgbm_experiment.list_trials():\n",
    "    proc_job = trial\n",
    "    break\n",
    "    \n",
    "lightgbm_tracker = Tracker.load(proc_job.trial_name)\n",
    "preprocessing_trial_component = lightgbm_tracker.trial_component\n",
    "print(preprocessing_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model with lightGBM\n",
    "\n",
    "There are multiple different ways to train a model in SageMaker.  One of the simplest ways to do so is to reuse the same container from above within SageMaker Processing itself to do the training.  This is possible due to the fact that the `ScriptProcessor` object we instantiated above can ingest an arbitrary Python script as long as we specify the input and output locations in S3.  \n",
    "\n",
    "An alternative is to use SageMaker hosted training.  Like SageMaker Processing, SageMaker hosted training spins up a right-sized, transient cluster for your job and then shuts it down when the job is done.  This enables you to do most of your work in lower-cost notebooks while reserving full scale training and related costs for only when you need it.  Using SageMaker hosted training offers several advantages over SageMaker Processing for training.  These include easy integrations with:  SageMaker Debugger, SageMaker Experiments, SageMaker Search, Managed Spot Training, Automatic Model Tuning, options for multiple file sources/channels with automated data shuffling and sharding, and more.  \n",
    "\n",
    "To use SageMaker hosted training, we'll create another simple Docker image.  We'll create another directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docker-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dockerfile for training is similar to the first one, with a few key differences:  \n",
    "- The parent image is from another ML framework's Docker image that bundles a bunch of necessary low-level build tools for the sagemaker-containers package (see next bullet point).  Another parent with those tools could be substituted.\n",
    "- There is one additional Python package:  sagemaker-containers, which integrates the container with SageMaker hosted training.\n",
    "- An environment variable indicating which Python module is the entry point for training.\n",
    "\n",
    "Note that you do NOT need to include the training script in the Docker image.  The sagemaker-containers package allows you to pass in a training script from an Amazon S3 location dynamically each time you start a training job, so you can reuse the same Docker image without rebuilding it for code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-train/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "RUN apt -y update && apt install -y --no-install-recommends \\\n",
    "    libgomp1 build-essential \\\n",
    "    && apt clean    \n",
    "RUN pip install lightgbm numpy pandas scikit-learn sagemaker-training\n",
    "ENV SAGEMAKER_PROGRAM train.py\n",
    "ENV PYTHONUNBUFFERED=TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a separate ECR repository for the training images, build the new training image, and push it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM python:3.7-slim-buster\n",
      " ---> 4d4a9832278b\n",
      "Step 2/5 : RUN apt -y update && apt install -y --no-install-recommends     libgomp1 build-essential     && apt clean\n",
      " ---> Using cache\n",
      " ---> 954365d3ec9c\n",
      "Step 3/5 : RUN pip install lightgbm numpy pandas scikit-learn sagemaker-training\n",
      " ---> Using cache\n",
      " ---> e0253bf05d41\n",
      "Step 4/5 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Using cache\n",
      " ---> c121d59106af\n",
      "Step 5/5 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> fc81428f0cf2\n",
      "Successfully built fc81428f0cf2\n",
      "Successfully tagged lightgbm-byo-train:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'lightgbm-byo-train' already exists in the registry with id '553020858742'\n",
      "The push refers to repository [553020858742.dkr.ecr.us-east-1.amazonaws.com/lightgbm-byo-train]\n",
      "\n",
      "\u001b[1Bf6eb0d47: Preparing \n",
      "\u001b[1Be7a5df1b: Preparing \n",
      "\u001b[1Be5632dc8: Preparing \n",
      "\u001b[1B857805ec: Preparing \n",
      "\u001b[1B87503449: Preparing \n",
      "\u001b[1B6688d36c: Preparing \n",
      "\u001b[1Bb4339852: Layer already exists \u001b[1A\u001b[2Klatest: digest: sha256:1aab483490d57e400c8ad99cd7b0cab562a5ae92fd87950147a30e8534957594 size: 1795\n"
     ]
    }
   ],
   "source": [
    "ecr_repository_train = 'lightgbm-byo-train'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "train_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository_train + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository_train docker-train\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository_train\n",
    "!docker tag {ecr_repository_train + tag} $train_repository_uri\n",
    "!docker push $train_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the training script.  Again, it is very similar to a Python script you would use outside SageMaker, and the main SageMaker-specific requirements are that you must specify several arguments from which you will extract hyperparameters such as the learning rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-train/train.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    # extract training data S3 location and hyperparameter values\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    parser.add_argument('--num_leaves', type=int, default=28)\n",
    "    parser.add_argument('--max_depth', type=int, default=5)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print('Loading training data from {}\\n'.format(args.train))\n",
    "    input_files = glob.glob('{}/*.npy'.format(args.train))\n",
    "    print('\\nTRAINING INPUT FILE LIST: \\n{}\\n'.format(input_files)) \n",
    "    for file in input_files:\n",
    "        if 'x_' in file:\n",
    "            x_train = np.load(file)\n",
    "        else:\n",
    "            y_train = np.load(file)      \n",
    "    print('\\nx_train shape: \\n{}\\n'.format(x_train.shape))\n",
    "    print('\\ny_train shape: \\n{}\\n'.format(y_train.shape))\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Loading validation data from {}\\n'.format(args.validation))\n",
    "    eval_input_files = glob.glob('{}/*.npy'.format(args.validation))\n",
    "    print('\\nVALIDATION INPUT FILE LIST: \\n{}\\n'.format(eval_input_files)) \n",
    "    for file in eval_input_files:\n",
    "        if 'x_' in file:\n",
    "            x_val = np.load(file)\n",
    "        else:\n",
    "            y_val = np.load(file)      \n",
    "    print('\\nx_val shape: \\n{}\\n'.format(x_val.shape))\n",
    "    print('\\ny_val shape: \\n{}\\n'.format(y_val.shape))\n",
    "    eval_data = lgb.Dataset(x_val, label=y_val)\n",
    "    \n",
    "    print('Training model with hyperparameters:\\n\\t num_leaves: {}\\n\\t max_depth: {}\\n\\t learning_rate: {}\\n'\n",
    "          .format(args.num_leaves, args.max_depth, args.learning_rate))\n",
    "    parameters = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'is_unbalance': 'true',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_leaves': args.num_leaves,\n",
    "        'max_depth': args.max_depth,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    num_round = 10\n",
    "    bst = lgb.train(parameters, train_data, num_round, eval_data, verbose_eval=1)\n",
    "    \n",
    "    print('Saving model . . . .')\n",
    "    bst.save_model('/opt/ml/model/online_shoppers_model.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script must be packaged as a .tar.gz file and uploaded to S3 for access by SageMaker.  This step must be repeated every time the script is modified, but avoids having to rebuild the Docker image for code changes:  you can just reuse the same Docker image with any lightGBM training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def create_tar_file(source_files, target=None):\n",
    "    if target:\n",
    "        filename = target\n",
    "    else:\n",
    "        _, filename = tempfile.mkstemp()\n",
    "\n",
    "    with tarfile.open(filename, mode=\"w:gz\") as t:\n",
    "        for sf in source_files:\n",
    "            t.add(sf, arcname=os.path.basename(sf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tar_file([\"docker-train/train.py\"], \"sourcedir.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/code/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "sources = session.upload_data('sourcedir.tar.gz', s3_output, s3_prefix + '/code')\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training script in Amazon S3, we can now set up an Amazon SageMaker Estimator object to represent the actual training job.  Similarly to the ScriptProcessor object, the Estimator takes in as parameters the Docker image, and instance type and amount.  Additionally, it takes in an encoded dictionary of hyperparameters for training.  For `train_instance_type` we specify `local`:  this allows us during the prototyping phase of a project to test SageMaker training code locally on the instance running this code.  Later in this example we will switch to a SageMaker instance type when we start multiple training jobs in parallel to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "import json\n",
    "\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "hyperparameters = json_encode_hyperparameters({\n",
    "    \"sagemaker_program\": \"train.py\",\n",
    "    \"sagemaker_submit_directory\": sources,\n",
    "    'num_leaves': 32,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.08})\n",
    "\n",
    "estimator = Estimator(image_uri='lightgbm-byo-train',\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='local',\n",
    "                      hyperparameters=hyperparameters,\n",
    "                      base_job_name='lightgbm-byo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method invocation starts the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: lightgbm-byo-2020-09-30-16-08-32-332\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-5likz:\n",
      "    command: train\n",
      "    environment:\n",
      "    - AWS_REGION=us-east-1\n",
      "    - TRAINING_JOB_NAME=lightgbm-byo-2020-09-30-16-08-32-332\n",
      "    image: lightgbm-byo-train\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-5likz\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpfwlsvnga/algo-1-5likz/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmpfwlsvnga/algo-1-5likz/output:/opt/ml/output\n",
      "    - /tmp/tmpfwlsvnga/algo-1-5likz/input:/opt/ml/input\n",
      "    - /tmp/tmpfwlsvnga/model:/opt/ml/model\n",
      "    - /tmp/tmpkvfw7ump:/opt/ml/input/data/train\n",
      "    - /tmp/tmp7nd2fb9r:/opt/ml/input/data/validation\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpfwlsvnga/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpfwlsvnga_algo-1-5likz_1 ... \n",
      "\u001b[1BAttaching to tmpfwlsvnga_algo-1-5likz_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m 2020-09-30 16:08:37,942 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m 2020-09-30 16:08:37,956 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m 2020-09-30 16:08:37,969 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m 2020-09-30 16:08:37,981 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"validation\": \"/opt/ml/input/data/validation\"\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"current_host\": \"algo-1-5likz\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"framework_module\": null,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"algo-1-5likz\"\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"num_leaves\": 32,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"max_depth\": 3,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"learning_rate\": 0.08\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"validation\": {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"job_name\": \"lightgbm-byo-2020-09-30-16-08-32-332\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"master_hostname\": \"algo-1-5likz\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/code/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"current_host\": \"algo-1-5likz\",\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m             \"algo-1-5likz\"\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_HOSTS=[\"algo-1-5likz\"]\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_HPS={\"learning_rate\":0.08,\"max_depth\":3,\"num_leaves\":32}\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-5likz\",\"hosts\":[\"algo-1-5likz\"]}\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_CHANNELS=[\"train\",\"validation\"]\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_CURRENT_HOST=algo-1-5likz\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_FRAMEWORK_MODULE=\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/code/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-5likz\",\"framework_module\":null,\"hosts\":[\"algo-1-5likz\"],\"hyperparameters\":{\"learning_rate\":0.08,\"max_depth\":3,\"num_leaves\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"lightgbm-byo-2020-09-30-16-08-32-332\",\"log_level\":20,\"master_hostname\":\"algo-1-5likz\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/code/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-5likz\",\"hosts\":[\"algo-1-5likz\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_USER_ARGS=[\"--learning_rate\",\"0.08\",\"--max_depth\",\"3\",\"--num_leaves\",\"32\"]\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_HP_NUM_LEAVES=32\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_HP_MAX_DEPTH=3\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m SM_HP_LEARNING_RATE=0.08\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m /usr/local/bin/python train.py --learning_rate 0.08 --max_depth 3 --num_leaves 32\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Loading training data from /opt/ml/input/data/train\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m TRAINING INPUT FILE LIST: \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m ['/opt/ml/input/data/train/y_train.npy', '/opt/ml/input/data/train/x_train.npy']\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m x_train shape: \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m (9864, 12)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m y_train shape: \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m (9864,)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Loading validation data from /opt/ml/input/data/validation\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m VALIDATION INPUT FILE LIST: \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m ['/opt/ml/input/data/validation/x_test.npy', '/opt/ml/input/data/validation/y_test.npy']\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m x_val shape: \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m (2466, 12)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m y_val shape: \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m (2466,)\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Training model with hyperparameters:\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \t num_leaves: 32\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \t max_depth: 3\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \t learning_rate: 0.08\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [LightGBM] [Info] Number of positive: 1526, number of negative: 8338\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m You can set `force_row_wise=true` to remove the overhead.\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m And if memory is not enough, you can set `force_col_wise=true`.\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [LightGBM] [Info] Total Bins 1827\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [LightGBM] [Info] Number of data points in the train set: 9864, number of used features: 12\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.154704 -> initscore=-1.698173\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [LightGBM] [Info] Start training from score -1.698173\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [1]\tvalid_0's binary_logloss: 0.394131\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [2]\tvalid_0's binary_logloss: 0.372158\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [3]\tvalid_0's binary_logloss: 0.35794\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [4]\tvalid_0's binary_logloss: 0.347911\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [5]\tvalid_0's binary_logloss: 0.340785\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [6]\tvalid_0's binary_logloss: 0.336069\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [7]\tvalid_0's binary_logloss: 0.332764\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [8]\tvalid_0's binary_logloss: 0.330338\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [9]\tvalid_0's binary_logloss: 0.329004\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m [10]\tvalid_0's binary_logloss: 0.328441\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m Saving model . . . .\n",
      "\u001b[36malgo-1-5likz_1  |\u001b[0m 2020-09-30 16:08:38,940 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpfwlsvnga_algo-1-5likz_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': preprocessed_training_data, 'validation': preprocessed_test_data}, logs='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily download the trained model, whether for further use inside of Amazon SageMaker or anywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-553020858742/lightgbm-byo-2020-09-30-16-08-32-332/model.tar.gz to model/model.tar.gz\n",
      "online_shoppers_model.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz\n",
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll upload the unzipped version of the model back to Amazon S3 for use by SageMaker Processing in model evaluation / batch scoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-553020858742/lightGBM-BYO/model/online_shoppers_model.txt\n"
     ]
    }
   ],
   "source": [
    "s3_model = session.upload_data('./model/online_shoppers_model.txt', s3_output, s3_prefix + '/model')\n",
    "print(s3_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutate the model / batch scoring\n",
    "\n",
    "Next we can reuse the Docker image from data preprocessing for model evaluation, or batch scoring.  Below is the evaluation script.  This time the main SageMaker-specific requirement is specifying an input directory.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import glob\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    print('Loading data . . . .')\n",
    "    input_files = glob.glob('{}/*.npy'.format('/opt/ml/processing/input'))\n",
    "    print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files)) \n",
    "    for file in input_files:\n",
    "        if 'x_' in file:\n",
    "            x_test = np.load(file)\n",
    "        else:\n",
    "            y_test = np.load(file)\n",
    "            \n",
    "    print('\\nx_test shape: \\n{}\\n'.format(x_test.shape))\n",
    "    print('\\ny_test shape: \\n{}\\n'.format(y_test.shape))\n",
    " \n",
    "    print('Loading model . . . .\\n')    \n",
    "    model_path = '/opt/ml/processing/model/'\n",
    "    bst_loaded = lgb.Booster(model_file=model_path+'online_shoppers_model.txt')\n",
    "    y_pred = bst_loaded.predict(x_test)\n",
    "    \n",
    "    print('Evaluating model . . . .\\n')    \n",
    "    acc = accuracy_score(y_test.astype(int), y_pred.round(0).astype(int))\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print('Accuracy:  {:.2f}'.format(acc))\n",
    "    print('AUC Score: {:.2f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also reuse the `ScriptProcessor` object we instantiated above, this time for the evaluation script.  Instead of having two outputs, as in the preprocessing job, there are two inputs:  one for the input data, and another for the model artifact to be used in the evaluation.  At the end of the job, we'll log the accuracy and AUC score metrics.  We also could have stored evaluation results to a file, or even saved visualization graphics, and asked SageMaker Processing to upload those to S3 at the end of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name lightgbm-byo-eval-30-16-09-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  lightgbm-byo-eval-30-16-09-04\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightGBM-BYO/data/test', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightGBM-BYO/model/online_shoppers_model.txt', 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightgbm-byo-eval-30-16-09-04/input/code/evaluation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'eval', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-553020858742/lightGBM-BYO/eval', 'LocalPath': '/opt/ml/processing/eval', 'S3UploadMode': 'EndOfJob'}}]\n",
      "......................\u001b[34mLoading data . . . .\n",
      "\u001b[0m\n",
      "\u001b[34mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/input/y_test.npy', '/opt/ml/processing/input/x_test.npy']\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mx_test shape: \u001b[0m\n",
      "\u001b[34m(2466, 12)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34my_test shape: \u001b[0m\n",
      "\u001b[34m(2466,)\n",
      "\u001b[0m\n",
      "\u001b[34mLoading model . . . .\n",
      "\u001b[0m\n",
      "\u001b[34mEvaluating model . . . .\n",
      "\u001b[0m\n",
      "\u001b[34mAccuracy:  0.88\u001b[0m\n",
      "\u001b[34mAUC Score: 0.90\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_job_name = \"lightgbm-byo-eval-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/eval'.format(s3_output, s3_prefix)\n",
    "\n",
    "script_processor.run(code='evaluation.py',\n",
    "                      job_name=processing_job_name,\n",
    "                      inputs=[ProcessingInput(\n",
    "                                source=preprocessed_test_data,\n",
    "                                destination='/opt/ml/processing/input'),\n",
    "                             ProcessingInput(\n",
    "                                source=s3_model,\n",
    "                                destination='/opt/ml/processing/model')],\n",
    "                      outputs=[ProcessingOutput(output_name='eval',\n",
    "                                                destination=output_destination,\n",
    "                                                source='/opt/ml/processing/eval')]\n",
    "                      )\n",
    "\n",
    "eval_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel training jobs:  Experiment with maximum tree depth\n",
    "\n",
    "Using the SageMaker Experiment we created earlier to track results, we will now experiment with the maximum tree depth hyperparameter of lightGBM.  To do this, we will start multiple SageMaker hosted training jobs in parallel with different values of the `max_depth` lightGBM hyperparameter.  Note that SageMaker also has an Automatic Model Tuning feature that enables you to do an automated, informed search over multiple hyperparameters using strategies such as Bayesian Optimization (which is the default).  \n",
    "\n",
    "In this code, note that we are again attaching an `experiment_config` to each job to automatically track results, and that we have defined an objective metric (validation loss) to track in the `metric_definitions` parameter of each training job that is launched.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: lightgbm-training-depth-3-1601483242\n",
      "INFO:sagemaker:Creating training-job with name: lightgbm-training-depth-6-1601483243\n",
      "INFO:sagemaker:Creating training-job with name: lightgbm-training-depth-9-1601483248\n",
      "INFO:sagemaker:Creating training-job with name: lightgbm-training-depth-12-1601483249\n"
     ]
    }
   ],
   "source": [
    "trial_name_map = {}\n",
    "\n",
    "for i, max_depth in enumerate([3, 6, 9, 12]):\n",
    "    # create trial\n",
    "    trial_name = f\"lightgbm-training-depth-{max_depth}-{int(time.time())}\"\n",
    "    trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "        experiment_name=lightgbm_experiment.experiment_name,\n",
    "        sagemaker_boto_client=boto3.client('sagemaker'),\n",
    "    )\n",
    "    trial_name_map[max_depth] = trial_name\n",
    "    # associate the proprocessing trial component with the current trial\n",
    "    trial.add_trial_component(preprocessing_trial_component)\n",
    "    \n",
    "    hyperparameters = json_encode_hyperparameters({ \"sagemaker_program\": \"train.py\",\n",
    "                                                    \"sagemaker_submit_directory\": sources,\n",
    "                                                    'num_leaves': 32,\n",
    "                                                    'max_depth': max_depth,\n",
    "                                                    'learning_rate': 0.08 })\n",
    "\n",
    "    estimator = Estimator(image_uri=train_repository_uri,\n",
    "                          role=role,\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.c4.8xlarge',\n",
    "                          hyperparameters=hyperparameters,\n",
    "                          enable_sagemaker_metrics=True,\n",
    "                          metric_definitions=[\n",
    "                            {'Name':'validation:loss', 'Regex':'.*loss: ([0-9\\\\.]+)'}\n",
    "                           ]\n",
    "                          )\n",
    "    \n",
    "    training_job_name = f\"lightgbm-training-depth-{max_depth}-{int(time.time())}\"\n",
    "    # Now associate the estimator with the Experiment and Trial\n",
    "    estimator.fit(\n",
    "        inputs={'train': preprocessed_training_data, 'validation': preprocessed_test_data}, \n",
    "        job_name=training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE CONTINUING WITH THE REST OF THIS NOTEBOOK DO THE FOLLOWING:**\n",
    "\n",
    "Go to the SageMaker console, and in the left panel click **Training jobs**.  You should see multiple training jobs with names of the form `lightgbm-training-depth-<number>-<time>`, with status **InProgress**.  Proceed with the rest of this notebook only when all of those jobs' status changes to **Completed**.\n",
    "\n",
    "Now we will use the analytics capabilities of the SageMaker Experiments Python SDK to query and compare the training runs in our experiment. You can retrieve specific trial components, such as training, by using a search expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll display the training trial components in ascending order of validation loss, which was used as a metric during training. Typically the trial components dataframe will have many columns. We can limit the number of columns displayed in various ways. For example, we can limit which parameter columns to show:  here only `max_depth` since it is the only one varying, while the others were fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(sess, sm), \n",
    "    experiment_name=lightgbm_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.validation:loss.Min\",\n",
    "    sort_order=\"Ascending\",\n",
    "    parameter_names=['max_depth']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at an example of tracing the lineage of a model by accessing the data tracked by SageMaker Experiments for the trial with `max_depth` = 9. This time the query also will return the preprocessing trial component, as well as the training component, so we can get a more complete picture of the steps taken to produce the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(sess, sm), \n",
    "    search_expression={\n",
    "        \"Filters\":[{\n",
    "            \"Name\": \"Parents.TrialName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": trial_name_map[9]\n",
    "        }]\n",
    "    },\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review a dataframe with this information.  Since the dataframe is merging information from two very different types of jobs, preprocessing and training, the original rows for each job will not have the same columns, so NaN is filled in for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_table.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to SageMaker\n",
    "\n",
    "There are several ways to deploy models within Amazon SageMaker.  For example, for offline batch use cases, it is possible to use either SageMaker Processing or SageMaker Batch Transform (which has some extra conveniences for very large scale jobs).  For real time prediction use cases, SageMaker hosted endpoints are applicable.  These offer many advantages including built-in options for A/B testing, autoscaling, and integration with SageMaker Model Monitor to detect data drift and other issues.\n",
    "\n",
    "In this example, we'll deploy the lightGBM model to an Amazon SageMaker hosted endpoint.  Again, there are multiple options for doing this, including using objects provided by the SageMaker Python SDK (such as the Estimator from above), or the AWS SDK for Python (boto3).  \n",
    "\n",
    "However, for convenience we'll use Ezsmdeploy, https://pypi.org/project/ezsmdeploy/.  Ezsmdeploy provides several conveniences such as automatically choosing an instance based on model size or based on a budget, enabling load testing endpoints using an intuitive API, an more.  This is especially a very convenient way to deploy a model when the model is:\n",
    "\n",
    "- a preexisting model that you trained outside Amazon SageMaker, or\n",
    "- trained within Amazon SageMaker but is simply an ordinary artifact, and has not yet been wrapped in a SageMaker Model object.  This is the case if you train models within SageMaker Processing.\n",
    "\n",
    "First we need to install Ezsmdeploy.  We'll also install lightGBM so we can test locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install ezsmdeploy lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model script and test locally\n",
    "\n",
    "Next, we need to supply Ezsmdeploy with a model script that contains `load_model()` and `predict()` functions. The first function is self-explanatory. For local testing, the second function allows sending a Numpy array payload instead of bytes, which is the actual input format for the model when deployed in Amazon SageMaker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile modelscript_lightgbm.py\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NUM_FEATURES = 12\n",
    "\n",
    "# return loaded model\n",
    "def load_model(modelpath):\n",
    "    \n",
    "    print('Model path:  {}'.format(modelpath))\n",
    "    model = lgb.Booster(model_file=os.path.join(modelpath,'online_shoppers_model.txt'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# return prediction based on loaded model (from the step above) and an input payload\n",
    "def predict(model, payload):\n",
    "    \n",
    "    print('Type of payload:  {}'.format(type(payload)))\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # locally, payload may come in as an np.ndarray\n",
    "        if type(payload)==np.ndarray:\n",
    "            out = model.predict(payload)\n",
    "            \n",
    "        # in remote / container based deployment, payload comes in as a stream of bytes\n",
    "        else:\n",
    "            data = np.frombuffer(payload, dtype=np.float64)\n",
    "            data = data.reshape((data.size // NUM_FEATURES, NUM_FEATURES))\n",
    "            out = model.predict(data)\n",
    "                \n",
    "    except Exception as e:\n",
    "        out = 'EXCEPTION: {}'.format(str(e))\n",
    "        \n",
    "    return out if type(out) is str else out.tobytes() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the `modelscript_lightgbm.py` script locally to make sure it is working correctly.  The output should be an array of floats representing prediction probabilities.  (The closer the number is to one, the more likely a purchase will be made.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscript_lightgbm import *\n",
    "import numpy as np\n",
    "\n",
    "x_test = np.load('./x_test.npy')\n",
    "print(x_test.shape)\n",
    "\n",
    "x_bytes = x_test.tobytes()\n",
    "\n",
    "model = load_model('./model') \n",
    "result = predict(model, x_bytes)\n",
    "print(type(result))\n",
    "print(np.frombuffer(result, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case there are other inference containers running in local mode, we'll stop existing containers to avoid conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop $(docker container ls -aq) >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try a local deployment in a container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezsmdeploy\n",
    "\n",
    "ez = ezsmdeploy.Deploy(model='./model',\n",
    "                       script='modelscript_lightgbm.py',\n",
    "                       requirements=['numpy','joblib','lightgbm'],\n",
    "                       instance_type='local',\n",
    "                       name='lightgbm-byo-deployment',\n",
    "                       wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test a payload against the container running locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ez.predictor.predict(x_test.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result comes back as bytes, which can be examined after decoding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.frombuffer(out, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in Amazon SageMaker\n",
    "\n",
    "Now that we have confirmed that everything is working locally, we can deploy to an Amazon SageMaker endpoint for real time predictions served by SageMaker-managed hardware for autoscaling, blue/green update deployments, and more.  \n",
    "\n",
    "The `Deploy` method invocation is very similar to the local one above.  The main difference is that we no longer specify `instance_type = 'local'`.  Instead, ezsmdeploy will choose an instance based on the total size of the model (or multiple models passed in), take into account the multiple workers per endpoint, and also optionally a “budget” that will choose `instance_type` based on a maximum acceptible cost per hour.  For details, see https://pypi.org/project/ezsmdeploy/#other-features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezonsm = ezsmdeploy.Deploy(model='./model',\n",
    "                           script='modelscript_lightgbm.py',\n",
    "                           requirements=['numpy','joblib','lightgbm'],\n",
    "                           name='lightgbm-byo-deployment',\n",
    "                           wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the local test, we can now test a payload against the container running on a SageMaker-managed endpoint.  The code is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_from_sm = ezonsm.predictor.predict(x_test.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can examine the result returned by the Amazon SageMaker endpoint.  It should be the same as the result returned by local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.frombuffer(out_from_sm, dtype=np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid charges for unneeded resources, be sure to delete the Amazon SageMaker endpoint you just created after you are finished with this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezonsm.predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(experiment):\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(sagemaker_boto_client=sm, trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                sagemaker_boto_client=sm,\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # tc is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "    experiment.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(lightgbm_experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
